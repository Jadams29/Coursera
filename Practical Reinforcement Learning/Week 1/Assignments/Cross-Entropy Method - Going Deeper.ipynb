{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_crossentropy_method.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVNjKppWR0LG",
        "colab_type": "text"
      },
      "source": [
        "# Deep Crossentropy method\n",
        "\n",
        "In this section we'll extend your CEM implementation with neural networks! You will train a multi-layer neural network to solve simple continuous state space games. __Please make sure you're done with tabular crossentropy method from the previous notebook.__\n",
        "\n",
        "![img](https://tip.duke.edu/independent_learning/greek/lesson/digging_deeper_final.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9M89CJHR0LH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week1_intro/submit.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXic2edsR0LJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# if you see \"<classname> has no attribute .env\", remove .env or update gym\n",
        "env = gym.make(\"CartPole-v0\").env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))\n",
        "print(\"state vector dim =\", state_dim)\n",
        "print(\"n_actions =\", n_actions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0FFqv5gR0LL",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network Policy\n",
        "\n",
        "For this assignment we'll utilize the simplified neural network implementation from __[Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)__. Here's what you'll need:\n",
        "\n",
        "* `agent.partial_fit(states, actions)` - make a single training pass over the data. Maximize the probabilitity of :actions: from :states:\n",
        "* `agent.predict_proba(states)` - predict probabilities of all actions, a matrix of shape __[len(states), n_actions]__\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB4cCLvjR0LM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "agent = MLPClassifier(\n",
        "    hidden_layer_sizes=(20, 20),\n",
        "    activation='tanh',\n",
        ")\n",
        "\n",
        "# initialize agent to the dimension of state space and number of actions\n",
        "agent.partial_fit([env.reset()] * n_actions, range(n_actions), range(n_actions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMmq8fuyR0LO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(env, agent, t_max=1000):\n",
        "    \"\"\"\n",
        "    Play a single game using agent neural network.\n",
        "    Terminate when game finishes or after :t_max: steps\n",
        "    \"\"\"\n",
        "    states, actions = [], []\n",
        "    total_reward = 0\n",
        "\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # use agent to predict a vector of action probabilities for state :s:\n",
        "        probs = agent.predict_proba([s]).reshape(n_actions)\n",
        "\n",
        "        assert probs.shape == (n_actions,), \"make sure probabilities are a vector (hint: np.reshape)\"\n",
        "        \n",
        "        # use the probabilities you predicted to pick an action\n",
        "        # sample proportionally to the probabilities, don't just take the most likely action\n",
        "        a = np.random.choice(np.arange(n_actions), p=probs)\n",
        "        # ^-- hint: try np.random.choice\n",
        "\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record sessions like you did before\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        total_reward += r\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "    return states, actions, total_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQSRIGZZR0LR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_states, dummy_actions, dummy_reward = generate_session(env, agent, t_max=5)\n",
        "print(\"states:\", np.stack(dummy_states))\n",
        "print(\"actions:\", dummy_actions)\n",
        "print(\"reward:\", dummy_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-sr2ADcR0LT",
        "colab_type": "text"
      },
      "source": [
        "### CEM steps\n",
        "Deep CEM uses exactly the same strategy as the regular CEM, so you can copy your function code from previous notebook.\n",
        "\n",
        "The only difference is that now each observation is not a number but a `float32` vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91b7S_wER0LT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "def select_elites(states_batch, actions_batch, rewards_batch, percentile):\n",
        "    \"\"\"\n",
        "    Select states and actions from games that have rewards >= percentile\n",
        "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
        "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
        "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
        "\n",
        "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
        "\n",
        "    Please return elite states and actions in their original order \n",
        "    [i.e. sorted by session number and timestep within session]\n",
        "\n",
        "    If you are confused, see examples below. Please don't assume that states are integers\n",
        "    (they will become different later).\n",
        "    \"\"\"\n",
        "    # reward_threshold = <YOUR CODE: compute minimum reward for elite sessions. Hint: use np.percentile()>\n",
        "\n",
        "    rewards_batch = np.asarray(rewards_batch)\n",
        "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
        "    reward_idx = np.where(rewards_batch >= np.ceil(reward_threshold))\n",
        "\n",
        "    elite_states = [states_batch[i] for i in reward_idx[0]]\n",
        "    elite_actions = [actions_batch[i] for i in reward_idx[0]]\n",
        "\n",
        "    return list(itertools.chain.from_iterable(elite_states)), list(itertools.chain.from_iterable(elite_actions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhX06cIGR0LW",
        "colab_type": "text"
      },
      "source": [
        "# Training loop\n",
        "Generate sessions, select N best and fit to those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWS7NccJR0LW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n",
        "    \"\"\"\n",
        "    A convenience function that displays training progress. \n",
        "    No cool math here, just charts.\n",
        "    \"\"\"\n",
        "\n",
        "    mean_reward = np.mean(rewards_batch)\n",
        "    threshold = np.percentile(rewards_batch, percentile)\n",
        "    log.append([mean_reward, threshold])\n",
        "\n",
        "    clear_output(True)\n",
        "    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n",
        "    plt.figure(figsize=[8, 4])\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
        "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(rewards_batch, range=reward_range)\n",
        "    plt.vlines([np.percentile(rewards_batch, percentile)],\n",
        "               [0], [100], label=\"percentile\", color='red')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-IFdO10R0LY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_sessions = 100\n",
        "percentile = 70\n",
        "log = []\n",
        "env.reset()\n",
        "\n",
        "for i in range(100):\n",
        "    # generate new sessions\n",
        "    sessions = [generate_session(env, agent, t_max=1000) for _ in range(n_sessions)]\n",
        "\n",
        "    states_batch, actions_batch, rewards_batch = map(np.array, zip(*sessions))\n",
        "\n",
        "    elite_states, elite_actions = select_elites(states_batch=states_batch, actions_batch=actions_batch, rewards_batch=rewards_batch, percentile=percentile)\n",
        "\n",
        "    # <YOUR CODE: partial_fit agent to predict elite_actions(y) from elite_states(X)>\n",
        "\n",
        "    agent.partial_fit(elite_states, elite_actions)\n",
        "\n",
        "    show_progress(rewards_batch, log, percentile, reward_range=[0, np.max(rewards_batch)])\n",
        "\n",
        "    if np.mean(rewards_batch) > 300:\n",
        "        print(\"You Win! You may stop training now via KeyboardInterrupt.\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJsCu7bmR0La",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY8SegEGR0Lb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor, agent) for _ in range(100)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK1PIJcRR0Ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices\n",
        "\n",
        "print(video_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-72k86VR0Lf",
        "colab_type": "text"
      },
      "source": [
        "## Assignment: MountainCar\n",
        "\n",
        "By this moment you should have got enough score on [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) to consider it solved (see the link). It's time to try something harder.\n",
        "\n",
        "_if you have any trouble with CartPole-v0 and feel stuck, take a look at the forums_\n",
        "\n",
        "Your assignment is to obtain average reward of __at least -150__ on `MountainCar-v0`.\n",
        "\n",
        "See the tips section below, it's kinda important.\n",
        "  \n",
        "* Bonus quest: Devise a way to speed up training against the default version\n",
        "  * Obvious improvement: use [joblib](https://www.google.com/search?client=ubuntu&channel=fs&q=joblib&ie=utf-8&oe=utf-8)\n",
        "  * Try re-using samples from 3-5 last iterations when computing threshold and training\n",
        "  * Experiment with amount of training iterations and learning rate of the neural network (see params)\n",
        "  \n",
        "  \n",
        "### Tips\n",
        "* Gym page: [MountainCar](https://gym.openai.com/envs/MountainCar-v0)\n",
        "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
        " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 10% are better, than if you use percentile 20% as threshold, R >= threshold __fails cut off bad sessions__ whule R > threshold works alright.\n",
        "* _issue with gym_: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified __t_max__, and if it isn't, try `env = gym.make(\"MountainCar-v0\").env` or otherwise get rid of TimeLimit wrapper.\n",
        "* If it won't train it's a good idea to plot reward distribution and record sessions: they may give you some clue. If they don't, call course staff :)\n",
        "* 20-neuron network is probably not enough, feel free to experiment.\n",
        "\n",
        "You may find the following snippet useful:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r50SBPHPR0Lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_mountain_car(env, agent):\n",
        "    xs = np.linspace(env.min_position, env.max_position, 100)\n",
        "    vs = np.linspace(-env.max_speed, env.max_speed, 100)\n",
        "    grid = np.dstack(np.meshgrid(xs, vs)).transpose(1, 0, 2)\n",
        "    grid_flat = grid.reshape(len(xs) * len(vs), 2)\n",
        "    probs = agent.predict_proba(grid_flat).reshape(len(xs), len(vs), 3)\n",
        "    return probs\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxF6SawGi4ex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "bd38fa38-c55b-46fe-d46e-f078f02bf0a4"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "env.reset()\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\").env\n",
        "agent = MLPClassifier(\n",
        "    hidden_layer_sizes=(40, 40, 20),\n",
        "    activation='tanh',)\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape[0]\n",
        "# initialize agent to the dimension of state space and number of actions\n",
        "agent.partial_fit([env.reset()] * n_actions, range(n_actions), range(n_actions))\n",
        "plt.imshow(visualize_mountain_car(env, agent))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f25e1ba1780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad_VpFhFR0Li",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement generate_session_mountain_car(), training loop, etc.\n",
        "\n",
        "def generate_session_mountain_car(env, agent, t_max=10000):\n",
        "    \"\"\"\n",
        "    Play a single game using agent neural network.\n",
        "    Terminate when game finishes or after :t_max: steps\n",
        "    \"\"\"\n",
        "    states, actions = [], []\n",
        "    total_reward = 0\n",
        "\n",
        "    s = env.reset()\n",
        "    for t in range(t_max):\n",
        "        # use agent to predict a vector of action probabilities for state :s:\n",
        "        # probs = agent.predict_proba([s]).reshape(n_actions)\n",
        "\n",
        "        assert probs.shape == (n_actions,), \"make sure probabilities are a vector (hint: np.reshape)\"\n",
        "        \n",
        "        # use the probabilities you predicted to pick an action\n",
        "        # sample proportionally to the probabilities, don't just take the most likely action\n",
        "        a = np.random.choice(np.arange(n_actions), p=probs)\n",
        "        # ^-- hint: try np.random.choice\n",
        "\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record sessions like you did before\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        total_reward += r\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break    \n",
        "    return states, actions, total_reward\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikB_6NYWhIUM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "53ab2e06-5019-4d8e-b1fb-89d35c94544c"
      },
      "source": [
        "import time\n",
        "\n",
        "n_sessions = 100\n",
        "percentile = 70\n",
        "log = []\n",
        "env.reset()\n",
        "\n",
        "%time temp = [generate_session_mountain_car(env, agent, t_max=10000) for _ in range(2)]\n",
        "\n",
        "# for i in range(100):\n",
        "#     # generate new sessions\n",
        "#     %time sessions = [generate_session_mountain_car(env, agent, t_max=100) for _ in range(n_sessions)]\n",
        "\n",
        "#     %time states_batch, actions_batch, rewards_batch = map(np.array, zip(*sessions))\n",
        "\n",
        "#     %time elite_states, elite_actions = select_elites(states_batch=states_batch, actions_batch=actions_batch, rewards_batch=rewards_batch, percentile=percentile)\n",
        "\n",
        "#     # <YOUR CODE: partial_fit agent to predict elite_actions(y) from elite_states(X)>\n",
        "\n",
        "#     print(np.mean(rewards_batch))\n",
        "#     agent.partial_fit(elite_states, elite_actions)\n",
        "#     # show_progress(rewards_batch, log, percentile, reward_range=[0, np.max(rewards_batch)])\n",
        "\n",
        "#     if np.mean(rewards_batch) > 10:\n",
        "#         print(\"You Win! You may stop training now via KeyboardInterrupt.\")\n",
        "#         break"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
            "Wall time: 4.53 µs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-a49efa4c485f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time temp = [generate_session_mountain_car(env, agent, t_max=10000) for _ in range(2)]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# for i in range(100):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2079\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2081\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-84b3b18f89bf>\u001b[0m in \u001b[0;36mgenerate_session_mountain_car\u001b[0;34m(env, agent, t_max)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# probs = agent.predict_proba([s]).reshape(n_actions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"make sure probabilities are a vector (hint: np.reshape)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# use the probabilities you predicted to pick an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'probs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI6bUrOmR0Lj",
        "colab_type": "text"
      },
      "source": [
        "### Submit to Coursera"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iibvv0VNR0Lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from submit import submit_mountain_car\n",
        "submit_mountain_car(generate_session_mountain_car, agent, 'your.email@example.com', 'YourAssignmentToken')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}